use mamba_parser::lexer::Lexer;
use mamba_parser::token::TokenKind;

#[test]
fn test_unicode_latin_identifier() {
    let mut lexer = Lexer::new("cafÃ© = 42");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("cafÃ©".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::Integer(42));
}

#[test]
fn test_unicode_greek_identifier() {
    let mut lexer = Lexer::new("Ï€ = 3.14");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("Ï€".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::Float(3.14));
}

#[test]
fn test_unicode_chinese_identifier() {
    let mut lexer = Lexer::new("æ•°æ® = 100");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("æ•°æ®".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::Integer(100));
}

#[test]
fn test_unicode_japanese_identifier() {
    let mut lexer = Lexer::new("å¤‰æ•° = 'hello'");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("å¤‰æ•°".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::String("hello".to_string()));
}

#[test]
fn test_unicode_arabic_identifier() {
    let mut lexer = Lexer::new("Ù…ØªØºÙŠØ± = 50");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("Ù…ØªØºÙŠØ±".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::Integer(50));
}

#[test]
fn test_unicode_cyrillic_identifier() {
    let mut lexer = Lexer::new("Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ = True");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::True);
}

#[test]
fn test_unicode_emoji_not_allowed() {
    // Emojis are not valid identifier characters in Python
    let mut lexer = Lexer::new("ğŸš€ = 42");
    let result = lexer.tokenize();
    
    // Should fail because ğŸš€ is not alphabetic
    assert!(result.is_err());
}

#[test]
fn test_unicode_mixed_identifier() {
    let mut lexer = Lexer::new("var_Ï€_2 = 6.28");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("var_Ï€_2".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::Float(6.28));
}

#[test]
fn test_unicode_underscore_start() {
    let mut lexer = Lexer::new("_Î¼ = 1");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("_Î¼".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::Integer(1));
}

#[test]
fn test_unicode_function_name() {
    let mut lexer = Lexer::new("def calculer_Ï€():");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Def);
    assert_eq!(tokens[1].kind, TokenKind::Identifier("calculer_Ï€".to_string()));
    assert_eq!(tokens[2].kind, TokenKind::LeftParen);
    assert_eq!(tokens[3].kind, TokenKind::RightParen);
    assert_eq!(tokens[4].kind, TokenKind::Colon);
}

#[test]
fn test_unicode_digit_in_identifier() {
    // Unicode digits (like Ù¢ Arabic-Indic digit 2) should work after the first char
    let mut lexer = Lexer::new("xÙ¢ = 5");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("xÙ¢".to_string()));
}

#[test]
fn test_unicode_cant_start_with_digit() {
    // Even Unicode digits can't start an identifier
    let mut lexer = Lexer::new("Ù¢x = 5");
    let result = lexer.tokenize();

    // The Arabic-Indic digit Ù¢ is considered a digit by Unicode,
    // but it's not ASCII, so our number tokenizer won't catch it
    // It will hit the "unexpected character" error in next_token
    assert!(result.is_err());
}

#[test]
fn test_unicode_whitespace_not_in_identifier() {
    let mut lexer = Lexer::new("var æ•°æ® = 1");
    let tokens = lexer.tokenize().unwrap();

    // Should be two separate identifiers
    assert_eq!(tokens[0].kind, TokenKind::Identifier("var".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Identifier("æ•°æ®".to_string()));
    assert_eq!(tokens[2].kind, TokenKind::Assign);
}

#[test]
fn test_unicode_mathematical_symbols() {
    let mut lexer = Lexer::new("Î” = 0.1");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("Î”".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
}

#[test]
fn test_unicode_various_scripts() {
    let code = "ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ = 1\nà¤¹à¤¿à¤¨à¤¦à¥€ = 2\ní•œêµ­ì–´ = 3";
    let mut lexer = Lexer::new(code);
    let tokens = lexer.tokenize().unwrap();

    // Greek
    assert_eq!(tokens[0].kind, TokenKind::Identifier("ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::Integer(1));
    assert_eq!(tokens[3].kind, TokenKind::Newline);
    
    // Hindi (simplified to avoid combining marks)
    assert_eq!(tokens[4].kind, TokenKind::Identifier("à¤¹à¤¿à¤¨à¤¦à¥€".to_string()));
    assert_eq!(tokens[5].kind, TokenKind::Assign);
    assert_eq!(tokens[6].kind, TokenKind::Integer(2));
    assert_eq!(tokens[7].kind, TokenKind::Newline);
    
    // Korean
    assert_eq!(tokens[8].kind, TokenKind::Identifier("í•œêµ­ì–´".to_string()));
    assert_eq!(tokens[9].kind, TokenKind::Assign);
    assert_eq!(tokens[10].kind, TokenKind::Integer(3));
}

#[test]
fn test_unicode_normalization_not_required() {
    // Different Unicode normalizations of cafÃ© (Ã© as single char vs e + combining accent)
    // Our lexer accepts them as-is without normalization
    let code1 = "cafÃ© = 1"; // Ã© as U+00E9
    let code2 = "cafÃ© = 1"; // e + Ì as U+0065 + U+0301
    
    let mut lexer1 = Lexer::new(code1);
    let tokens1 = lexer1.tokenize().unwrap();
    
    let mut lexer2 = Lexer::new(code2);
    let tokens2 = lexer2.tokenize().unwrap();
    
    // Both should tokenize successfully (identifiers will be different strings though)
    assert!(matches!(tokens1[0].kind, TokenKind::Identifier(_)));
    assert!(matches!(tokens2[0].kind, TokenKind::Identifier(_)));
}

#[test]
fn test_ascii_still_works() {
    // Make sure we didn't break ASCII identifiers
    let mut lexer = Lexer::new("hello_world_123 = 'test'");
    let tokens = lexer.tokenize().unwrap();

    assert_eq!(tokens[0].kind, TokenKind::Identifier("hello_world_123".to_string()));
    assert_eq!(tokens[1].kind, TokenKind::Assign);
    assert_eq!(tokens[2].kind, TokenKind::String("test".to_string()));
}
